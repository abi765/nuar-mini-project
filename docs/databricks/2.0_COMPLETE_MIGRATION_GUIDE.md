# ðŸš€ Databricks Migration Guide

**Complete guide to deploying NUAR Mini project to Databricks**

---

## ðŸ“‹ Table of Contents

1. [Prerequisites](#prerequisites)
2. [Databricks Workspace Setup](#databricks-workspace-setup)
3. [Repository Integration](#repository-integration)
4. [Secret Management](#secret-management)
5. [Cluster Configuration](#cluster-configuration)
6. [Delta Lake Setup](#delta-lake-setup)
7. [Notebook Deployment](#notebook-deployment)
8. [Workflow Orchestration](#workflow-orchestration)
9. [Cost Optimization](#cost-optimization)
10. [Testing & Validation](#testing--validation)

---

## Prerequisites

### Required Accounts

- âœ… **Databricks Account** (Community Edition or paid tier)
  - Sign up: [https://community.cloud.databricks.com/login.html](https://community.cloud.databricks.com/login.html)
- âœ… **GitHub Account** for repository integration
- âœ… **OpenWeatherMap API Key** (free tier)
  - Get key: [https://openweathermap.org/api](https://openweathermap.org/api)

### Local Prerequisites

```bash
# Databricks CLI (optional but recommended)
pip install databricks-cli

# Configure Databricks CLI
databricks configure --token
```

---

## Databricks Workspace Setup

### 1. Create Workspace

**For Community Edition:**

1. Go to [https://community.cloud.databricks.com](https://community.cloud.databricks.com)
2. Sign up or log in
3. Your workspace is automatically created

**For Paid Tier:**

1. Log into your Databricks account
2. Create a new workspace
3. Select region closest to you (for Stockport data: Europe West 2 - London recommended)

### 2. Create Catalog and Schemas

Run this in a Databricks notebook:

```sql
-- Create catalog for NUAR project
CREATE CATALOG IF NOT EXISTS nuar_catalog;

USE CATALOG nuar_catalog;

-- Create schemas for each layer
CREATE SCHEMA IF NOT EXISTS bronze
  COMMENT 'Raw data from APIs - Parquet format';

CREATE SCHEMA IF NOT EXISTS silver
  COMMENT 'Cleaned and transformed data - Delta format';

CREATE SCHEMA IF NOT EXISTS gold
  COMMENT 'Aggregated analytics - Delta format';

-- Verify creation
SHOW SCHEMAS IN nuar_catalog;
```

---

## Repository Integration

### Method 1: Databricks Repos (Recommended)

1. **Navigate to Repos**
   - Click on "Repos" in the left sidebar
   - Click "Add Repo"

2. **Connect GitHub Repository**
   ```
   Git repo URL: https://github.com/YOUR_USERNAME/nuar_mini_project
   Git provider: GitHub
   Repo name: nuar_mini_project
   ```

3. **Clone Repository**
   - Click "Create Repo"
   - Your entire project structure is now in Databricks

4. **Benefits**:
   - âœ… Version control integration
   - âœ… Pull latest changes easily
   - âœ… Collaborative development
   - âœ… All notebooks and code in one place

### Method 2: Manual Upload

1. **Create Folders**
   - Navigate to Workspace
   - Create folder structure: `/Workspace/Users/YOUR_EMAIL/nuar_mini_project/`

2. **Upload Files**
   - Upload notebooks individually
   - Upload Python files to DBFS

---

## Secret Management

### 1. Create Secret Scope

**Using Databricks CLI:**

```bash
# Create secret scope
databricks secrets create-scope --scope nuar_secrets

# Add OpenWeatherMap API key
databricks secrets put --scope nuar_secrets --key openweather_api_key
# This will open an editor - paste your API key and save
```

**Using Databricks UI** (Community Edition):

```python
# In a notebook cell
dbutils.secrets.help()

# Note: Community Edition has limited secret scope features
# You may need to use environment variables instead
```

### 2. Access Secrets in Notebooks

```python
# In your Databricks notebooks
OPENWEATHER_API_KEY = dbutils.secrets.get(scope="nuar_secrets", key="openweather_api_key")

# Or use environment variables
import os
os.environ['OPENWEATHER_API_KEY'] = dbutils.secrets.get(scope="nuar_secrets", key="openweather_api_key")
```

### 3. Update Configuration Files

Create a Databricks-specific settings file:

**`config/databricks_settings.py`:**

```python
"""
Databricks-specific configuration
"""
import os

# API Keys (from secrets)
try:
    OPENWEATHER_API_KEY = dbutils.secrets.get(scope="nuar_secrets", key="openweather_api_key")
except:
    OPENWEATHER_API_KEY = os.getenv('OPENWEATHER_API_KEY')

# Databricks catalog and schema settings
CATALOG_NAME = "nuar_catalog"
BRONZE_SCHEMA = "bronze"
SILVER_SCHEMA = "silver"
GOLD_SCHEMA = "gold"

# DBFS paths (replace local paths)
BRONZE_BASE_PATH = f"dbfs:/FileStore/nuar/{BRONZE_SCHEMA}"
SILVER_BASE_PATH = f"dbfs:/FileStore/nuar/{SILVER_SCHEMA}"
GOLD_BASE_PATH = f"dbfs:/FileStore/nuar/{GOLD_SCHEMA}"

# API Endpoints (same as local)
OVERPASS_ENDPOINT = "https://overpass-api.de/api/interpreter"
POLICE_API_BASE = "https://data.police.uk/api"
WEATHER_API_BASE = "https://api.openweathermap.org/data/2.5"
POSTCODES_API_BASE = "https://api.postcodes.io"

# Stockport configuration
from config.stockport import STOCKPORT_CONFIG
STOCKPORT_BBOX = STOCKPORT_CONFIG['stockport']['bounding_box']
STOCKPORT_CENTER = STOCKPORT_CONFIG['stockport']['center']
```

---

## Cluster Configuration

### 1. Create All-Purpose Cluster

**Recommended Configuration for NUAR Project:**

```yaml
Cluster Name: nuar-analysis-cluster
Cluster Mode: Single Node (for development)
Databricks Runtime: 14.3 LTS (includes Python 3.10, Spark 3.5.0)
Node Type:
  - Community Edition: Standard_DS3_v2 (default)
  - Paid: i3.xlarge (AWS) or Standard_DS3_v2 (Azure)
Autopilot Options:
  - Enable autoscaling: OFF (for predictable costs)
  - Terminate after: 120 minutes of inactivity
```

### 2. Install Libraries

**Method A: Cluster Libraries UI**

1. Go to your cluster
2. Click "Libraries" tab
3. Click "Install New"
4. Install from PyPI:
   ```
   pyarrow
   geopandas
   pyproj
   shapely
   scipy
   python-dotenv
   requests
   ```

**Method B: Notebook Installation**

```python
# Run in first cell of each notebook
%pip install pyarrow geopandas pyproj shapely scipy python-dotenv requests

# Restart Python to load new packages
dbutils.library.restartPython()
```

**Method C: Init Script** (Recommended for production)

Create init script at `dbfs:/databricks/scripts/install_libraries.sh`:

```bash
#!/bin/bash
/databricks/python/bin/pip install pyarrow geopandas pyproj shapely scipy python-dotenv requests
```

Attach to cluster in Advanced Options â†’ Init Scripts

---

## Delta Lake Setup

### 1. Create Bronze Layer Tables

```python
# Bronze Infrastructure Table
spark.sql("""
  CREATE TABLE IF NOT EXISTS nuar_catalog.bronze.infrastructure (
    id BIGINT,
    type STRING,
    lat DOUBLE,
    lon DOUBLE,
    infrastructure_type STRING,
    substance STRING,
    location STRING,
    operator STRING,
    name STRING,
    diameter STRING,
    material STRING,
    all_tags STRING,
    node_count INT,
    geometry_points INT,
    first_lat DOUBLE,
    first_lon DOUBLE,
    last_lat DOUBLE,
    last_lon DOUBLE,
    geometry_json STRING,
    ingestion_timestamp TIMESTAMP,
    ingestion_date DATE,
    area STRING
  )
  USING DELTA
  PARTITIONED BY (ingestion_date, infrastructure_type)
  LOCATION 'dbfs:/FileStore/nuar/bronze/infrastructure'
  COMMENT 'Raw infrastructure data from Overpass API'
""")

# Bronze Crime Table
spark.sql("""
  CREATE TABLE IF NOT EXISTS nuar_catalog.bronze.crime (
    crime_id STRING,
    persistent_id STRING,
    category STRING,
    month DATE,
    lat DOUBLE,
    lon DOUBLE,
    street_id STRING,
    street_name STRING,
    location_type STRING,
    location_subtype STRING,
    outcome_category STRING,
    outcome_date STRING,
    context STRING,
    ingestion_timestamp TIMESTAMP,
    ingestion_date DATE,
    area STRING
  )
  USING DELTA
  PARTITIONED BY (ingestion_date, month)
  LOCATION 'dbfs:/FileStore/nuar/bronze/crime'
  COMMENT 'Raw crime data from UK Police API'
""")

# Bronze Weather Table
spark.sql("""
  CREATE TABLE IF NOT EXISTS nuar_catalog.bronze.weather (
    location_name STRING,
    country STRING,
    lat DOUBLE,
    lon DOUBLE,
    datetime TIMESTAMP,
    timezone_offset INT,
    weather_id INT,
    weather_main STRING,
    weather_description STRING,
    weather_icon STRING,
    temp_celsius DOUBLE,
    feels_like_celsius DOUBLE,
    temp_min_celsius DOUBLE,
    temp_max_celsius DOUBLE,
    pressure_hpa DOUBLE,
    humidity_percent DOUBLE,
    visibility_meters DOUBLE,
    wind_speed_ms DOUBLE,
    wind_direction_deg DOUBLE,
    wind_gust_ms DOUBLE,
    clouds_percent DOUBLE,
    sunrise TIMESTAMP,
    sunset TIMESTAMP,
    ingestion_timestamp TIMESTAMP,
    ingestion_date DATE,
    area STRING
  )
  USING DELTA
  PARTITIONED BY (ingestion_date)
  LOCATION 'dbfs:/FileStore/nuar/bronze/weather'
  COMMENT 'Raw weather snapshots from OpenWeatherMap'
""")
```

### 2. Create Silver Layer Tables

```python
# Silver Infrastructure Table
spark.sql("""
  CREATE TABLE IF NOT EXISTS nuar_catalog.silver.infrastructure (
    id BIGINT,
    type STRING,
    lat DOUBLE,
    lon DOUBLE,
    easting_bng BIGINT,
    northing_bng BIGINT,
    infrastructure_type STRING,
    substance STRING,
    location STRING,
    operator STRING,
    name STRING,
    diameter STRING,
    material STRING,
    length_meters DOUBLE,
    postcode STRING,
    admin_district STRING,
    admin_ward STRING,
    has_coordinates BOOLEAN,
    has_bng_coords BOOLEAN,
    has_postcode BOOLEAN,
    has_admin_data BOOLEAN,
    is_valid_location BOOLEAN,
    transformation_timestamp TIMESTAMP,
    transformation_date DATE,
    data_layer STRING,
    source_system STRING,
    ingestion_date DATE
  )
  USING DELTA
  PARTITIONED BY (ingestion_date, infrastructure_type)
  LOCATION 'dbfs:/FileStore/nuar/silver/infrastructure'
  COMMENT 'Cleaned and transformed infrastructure data with BNG coordinates'
""")
```

---

## Notebook Deployment

### 1. Convert Python Scripts to Databricks Notebooks

Your current `.py` files need minor modifications:

**Changes needed:**

```python
# OLD (local): Reading from local filesystem
df = pd.read_parquet('data/bronze/stockport/infrastructure/parquet/infrastructure_stockport')

# NEW (Databricks): Reading from DBFS or Delta tables
df = spark.table("nuar_catalog.bronze.infrastructure").toPandas()
# OR
df = pd.read_parquet('/dbfs/FileStore/nuar/bronze/infrastructure')

# OLD (local): Importing from src/
from src.utils.api_client import OverpassClient

# NEW (Databricks): Use %run to import
%run ./src/utils/api_client

# OR install as package (see below)
```

### 2. Create Databricks-Specific Notebooks

**Example: `databricks/01_bronze_infrastructure.py`**

```python
# Databricks notebook source
# MAGIC %md
# MAGIC # Bronze Layer - Infrastructure Data Ingestion
# MAGIC
# MAGIC Collects infrastructure data from Overpass API and saves to Delta Lake

# COMMAND ----------
# Install required libraries
%pip install pyarrow geopandas pyproj shapely scipy requests
dbutils.library.restartPython()

# COMMAND ----------
# Import modules
%run ../src/utils/api_client
%run ../src/bronze/ingestion

# Or use sys.path approach
import sys
sys.path.append('/Workspace/Repos/YOUR_EMAIL/nuar_mini_project')
from src.utils.api_client import OverpassClient
from src.bronze.ingestion import save_to_bronze_parquet, flatten_overpass_elements

# COMMAND ----------
# Get API configuration
import json

with open('/Workspace/Repos/YOUR_EMAIL/nuar_mini_project/config/stockport.json') as f:
    stockport_config = json.load(f)

bbox = stockport_config['stockport']['bounding_box']
infrastructure_types = stockport_config['stockport']['infrastructure_focus']

# COMMAND ----------
# Query Overpass API
client = OverpassClient(timeout=300)
result = client.query_infrastructure(bbox, infrastructure_types)

# COMMAND ----------
# Flatten and save to Delta
if result:
    elements = result.get('elements', [])
    flattened = flatten_overpass_elements(elements)

    # Convert to Spark DataFrame
    import pandas as pd
    from datetime import datetime

    df_pd = pd.DataFrame(flattened)
    df_pd['ingestion_timestamp'] = datetime.now()
    df_pd['ingestion_date'] = datetime.now().date()
    df_pd['area'] = 'stockport'

    # Write to Delta table
    df_spark = spark.createDataFrame(df_pd)
    df_spark.write \
        .format("delta") \
        .mode("append") \
        .partitionBy("ingestion_date", "infrastructure_type") \
        .saveAsTable("nuar_catalog.bronze.infrastructure")

    print(f"âœ… Saved {len(df_pd)} infrastructure records to Delta Lake")
else:
    print("âŒ No data retrieved from API")

# COMMAND ----------
# Verify data
display(spark.table("nuar_catalog.bronze.infrastructure").limit(10))
```

### 3. Directory Structure in Databricks Repos

```text
/Repos/YOUR_EMAIL/nuar_mini_project/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ stockport.json
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ databricks_settings.py (new)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ api_client.py
â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â””â”€â”€ ingestion.py
â”‚   â””â”€â”€ silver/
â”‚       â””â”€â”€ transformations.py
â”œâ”€â”€ databricks_notebooks/  (new folder for Databricks versions)
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”œâ”€â”€ 01_infrastructure.py
â”‚   â”‚   â”œâ”€â”€ 02_crime.py
â”‚   â”‚   â”œâ”€â”€ 03_weather.py
â”‚   â”‚   â””â”€â”€ 04_postcodes.py
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ 05_infrastructure.py
â”‚   â”‚   â”œâ”€â”€ 06_crime.py
â”‚   â”‚   â””â”€â”€ 07_weather.py
â”‚   â””â”€â”€ gold/
â”‚       â””â”€â”€ 08_analytics.py
â””â”€â”€ docs/
    â””â”€â”€ DATABRICKS_MIGRATION.md (this file)
```

---

## Workflow Orchestration

### 1. Create Databricks Workflow (Jobs)

**Navigate to Workflows:**

1. Click "Workflows" in sidebar
2. Click "Create Job"
3. Configure job:

```yaml
Job Name: NUAR Daily Data Pipeline
Cluster: nuar-analysis-cluster

Tasks:
  1. Bronze_Infrastructure:
       Type: Notebook
       Path: /Repos/YOUR_EMAIL/nuar_mini_project/databricks_notebooks/bronze/01_infrastructure
       Timeout: 30 minutes

  2. Bronze_Crime:
       Type: Notebook
       Path: /Repos/YOUR_EMAIL/nuar_mini_project/databricks_notebooks/bronze/02_crime
       Depends on: Bronze_Infrastructure
       Timeout: 30 minutes

  3. Bronze_Weather:
       Type: Notebook
       Path: /Repos/YOUR_EMAIL/nuar_mini_project/databricks_notebooks/bronze/03_weather
       Depends on: Bronze_Crime
       Timeout: 10 minutes

  4. Bronze_Postcodes:
       Type: Notebook
       Path: /Repos/YOUR_EMAIL/nuar_mini_project/databricks_notebooks/bronze/04_postcodes
       Depends on: Bronze_Weather
       Timeout: 10 minutes

  5. Silver_Infrastructure:
       Type: Notebook
       Path: /Repos/YOUR_EMAIL/nuar_mini_project/databricks_notebooks/silver/05_infrastructure
       Depends on: Bronze_Postcodes
       Timeout: 20 minutes

  6. Silver_Crime:
       Type: Notebook
       Path: /Repos/YOUR_EMAIL/nuar_mini_project/databricks_notebooks/silver/06_crime
       Depends on: Silver_Infrastructure
       Timeout: 20 minutes

  7. Silver_Weather:
       Type: Notebook
       Path: /Repos/YOUR_EMAIL/nuar_mini_project/databricks_notebooks/silver/07_weather
       Depends on: Silver_Crime
       Timeout: 10 minutes

Schedule:
  Trigger: Scheduled
  Cron: 0 2 * * * (daily at 2 AM)
  Timezone: Europe/London

Notifications:
  Email on failure: YOUR_EMAIL@example.com
```

### 2. Manual Run vs Scheduled

**Manual Run:**

```python
# In a notebook
dbutils.notebook.run(
    "/Repos/YOUR_EMAIL/nuar_mini_project/databricks_notebooks/bronze/01_infrastructure",
    timeout_seconds=1800
)
```

**Scheduled via Workflow:**

- Set up in Workflows UI
- Define dependencies between tasks
- Configure retry logic and alerts

---

## Cost Optimization

### 1. Cluster Cost Management

**Community Edition:**

- âœ… FREE tier available
- âš ï¸ Limited to 15 GB cluster
- âš ï¸ 2-hour idle timeout

**Paid Tier Optimization:**

```yaml
# Development Cluster (lower cost)
Node Type: r5.large (AWS) or Standard_D3_v2 (Azure)
Workers: 0 (Single node mode)
Autoscaling: Disabled
Auto-termination: 30 minutes

# Production Cluster (optimized performance)
Node Type: i3.xlarge (AWS) or Standard_DS3_v2 (Azure)
Workers: 2-4 (with autoscaling)
Autoscaling: Enabled
Auto-termination: 120 minutes
Spot instances: Enabled (for non-critical workloads)
```

### 2. Storage Cost Management

```python
# Use Z-Ordering for faster queries
spark.sql("""
  OPTIMIZE nuar_catalog.silver.infrastructure
  ZORDER BY (infrastructure_type, easting_bng, northing_bng)
""")

# Vacuum old files (removes historical versions)
spark.sql("""
  VACUUM nuar_catalog.silver.infrastructure RETAIN 168 HOURS
""")

# Monitor table sizes
display(spark.sql("DESCRIBE DETAIL nuar_catalog.silver.infrastructure"))
```

### 3. Query Optimization

```python
# Use caching for frequently accessed tables
spark.sql("CACHE TABLE nuar_catalog.silver.infrastructure")

# Partition pruning in queries
spark.sql("""
  SELECT * FROM nuar_catalog.silver.infrastructure
  WHERE ingestion_date >= '2025-10-01'
  AND infrastructure_type = 'manhole'
""")
```

### 4. Cost Monitoring

```sql
-- Check table sizes
SELECT
  table_catalog,
  table_schema,
  table_name,
  table_type,
  size_bytes / 1024 / 1024 AS size_mb
FROM system.information_schema.tables
WHERE table_catalog = 'nuar_catalog'
ORDER BY size_bytes DESC;
```

---

## Testing & Validation

### 1. Smoke Tests

**Create notebook: `tests/databricks_smoke_test.py`**

```python
# Databricks notebook source
# MAGIC %md
# MAGIC # Databricks Environment Smoke Test

# COMMAND ----------
# Test 1: Catalog exists
try:
    spark.sql("USE CATALOG nuar_catalog")
    print("âœ… Catalog access OK")
except Exception as e:
    print(f"âŒ Catalog error: {e}")

# COMMAND ----------
# Test 2: Schemas exist
schemas = spark.sql("SHOW SCHEMAS IN nuar_catalog").collect()
required_schemas = ['bronze', 'silver', 'gold']
existing_schemas = [row.databaseName for row in schemas]

for schema in required_schemas:
    if schema in existing_schemas:
        print(f"âœ… Schema {schema} exists")
    else:
        print(f"âŒ Schema {schema} missing")

# COMMAND ----------
# Test 3: API connectivity
import requests

apis = {
    "Overpass": "https://overpass-api.de/api/status",
    "Police": "https://data.police.uk/api/forces",
    "Postcodes": "https://api.postcodes.io/postcodes/M1+1AE"
}

for name, url in apis.items():
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            print(f"âœ… {name} API accessible")
        else:
            print(f"âš ï¸  {name} API returned {response.status_code}")
    except Exception as e:
        print(f"âŒ {name} API error: {e}")

# COMMAND ----------
# Test 4: Secret access
try:
    api_key = dbutils.secrets.get(scope="nuar_secrets", key="openweather_api_key")
    if api_key and len(api_key) > 0:
        print("âœ… OpenWeather API key accessible")
    else:
        print("âŒ API key is empty")
except Exception as e:
    print(f"âŒ Secret access error: {e}")

# COMMAND ----------
# Test 5: Library imports
try:
    import pandas as pd
    import geopandas as gpd
    from pyproj import Transformer
    from shapely.geometry import Point
    print("âœ… All required libraries installed")
except ImportError as e:
    print(f"âŒ Missing library: {e}")
```

### 2. Data Quality Validation

```python
# Validate Bronze layer data
bronze_infra = spark.table("nuar_catalog.bronze.infrastructure")

# Check record count
count = bronze_infra.count()
print(f"Infrastructure records: {count}")
assert count > 0, "No infrastructure data found"

# Check for nulls in critical fields
null_coords = bronze_infra.filter("lat IS NULL OR lon IS NULL").count()
print(f"Records with missing coordinates: {null_coords}")

# Check date partitions
display(bronze_infra.groupBy("ingestion_date", "infrastructure_type").count())
```

---

## Troubleshooting

### Common Issues

**1. Import Errors**

```python
# Solution: Use %run magic command
%run ../src/utils/api_client

# Or add to path
import sys
sys.path.append('/Workspace/Repos/YOUR_EMAIL/nuar_mini_project')
```

**2. DBFS Permission Errors**

```python
# Use dbutils for file operations
dbutils.fs.mkdirs("/FileStore/nuar/bronze")
dbutils.fs.ls("/FileStore/nuar/")
```

**3. Delta Table Not Found**

```sql
-- Create table if missing
CREATE TABLE IF NOT EXISTS nuar_catalog.bronze.infrastructure ...

-- Or use saveAsTable with mode
df.write.format("delta").mode("append").saveAsTable("nuar_catalog.bronze.infrastructure")
```

**4. API Timeout in Databricks**

```python
# Increase timeout for Overpass queries
client = OverpassClient(timeout=600)  # 10 minutes

# Use try-except with retries
import time
for attempt in range(3):
    try:
        result = client.query_infrastructure(bbox, types)
        break
    except Exception as e:
        print(f"Attempt {attempt + 1} failed: {e}")
        time.sleep(60)
```

---

## Next Steps After Migration

1. âœ… **Run smoke tests** to validate environment
2. âœ… **Execute Bronze layer notebooks** to collect initial data
3. âœ… **Run Silver layer notebooks** to transform data
4. âœ… **Verify Delta tables** have correct data
5. âœ… **Set up scheduled workflows** for automation
6. âœ… **Create dashboards** using Databricks SQL
7. âœ… **Implement Gold layer** analytics

---

## Additional Resources

- [Databricks Documentation](https://docs.databricks.com/)
- [Delta Lake Guide](https://docs.delta.io/latest/index.html)
- [Databricks Repos](https://docs.databricks.com/repos/index.html)
- [Databricks Workflows](https://docs.databricks.com/workflows/index.html)
- [Cost Management](https://docs.databricks.com/administration-guide/account-settings/cost-management.html)

---

**Last Updated**: 2025-10-27
**Version**: 1.0.0
**Status**: Production Ready ðŸš€
