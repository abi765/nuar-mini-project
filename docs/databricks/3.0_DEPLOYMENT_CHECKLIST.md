# ðŸš€ Databricks Deployment Checklist

**Complete checklist for migrating NUAR Mini to Databricks**

---

## Pre-Deployment (Local)

### âœ… Prerequisites

- [ ] Python 3.8+ installed
- [ ] Git repository initialized
- [ ] `.env` file with OpenWeather API key configured
- [ ] All notebooks tested locally
- [ ] Data quality validated

### âœ… Code Preparation

- [x] README.md updated with Databricks migration info
- [x] Databricks migration guide created (`docs/DATABRICKS_MIGRATION.md`)
- [x] Databricks configuration file created (`config/databricks_settings.py`)
- [x] Setup script created and tested (`databricks_setup.sh`)
- [x] Sample Databricks notebooks created (`databricks_notebooks/`)
- [x] `.gitignore` updated for Databricks

### âœ… Push to GitHub

```bash
# Review changes
git status

# Add all files
git add .

# Commit
git commit -m "feat: Add Databricks deployment configuration and migration guide"

# Push to GitHub
git push origin main
```

---

## Databricks Setup

### Step 1: Create Workspace

- [ ] Sign up/log in to Databricks
  - Community: [https://community.cloud.databricks.com](https://community.cloud.databricks.com)
  - Or use your organization's workspace
- [ ] Workspace URL saved: `https://_____________.databricks.com`
- [ ] User email noted: `____________@________.com`

### Step 2: Set Up Repository

- [ ] Navigate to "Repos" in Databricks sidebar
- [ ] Click "Add Repo"
- [ ] Enter GitHub repository URL: `https://github.com/YOUR_USERNAME/nuar_mini_project`
- [ ] Repo successfully cloned
- [ ] Verify file structure matches local

### Step 3: Configure Secrets

**Using Databricks CLI (Recommended):**

```bash
# Install CLI
pip install databricks-cli

# Configure with token
databricks configure --token
# Enter: Databricks URL, Access Token

# Create secret scope
databricks secrets create-scope --scope nuar_secrets

# Add API key
databricks secrets put --scope nuar_secrets --key openweather_api_key
# Paste your API key when editor opens
```

**Or using UI:**

- [ ] Navigate to Settings â†’ Secrets (if available in your tier)
- [ ] Create scope: `nuar_secrets`
- [ ] Add secret: `openweather_api_key` = `YOUR_API_KEY`

### Step 4: Create Cluster

**Cluster Configuration:**

```yaml
Name: nuar-analysis-cluster
Policy: Unrestricted
Mode: Single Node
Databricks Runtime: 14.3 LTS (Recommended)
Node Type:
  - Community: Standard_DS3_v2 (default)
  - Paid: i3.xlarge or Standard_DS3_v2
Autopilot: Enabled
Autoscaling: Disabled
Terminate after: 120 minutes of inactivity
```

- [ ] Cluster created
- [ ] Cluster started successfully
- [ ] Note cluster ID: `_________________`

### Step 5: Install Libraries

**Option A: Cluster Libraries (Recommended)**

- [ ] Go to cluster â†’ Libraries tab
- [ ] Install from PyPI:
  - [ ] `pyarrow`
  - [ ] `geopandas`
  - [ ] `pyproj`
  - [ ] `shapely`
  - [ ] `scipy`
  - [ ] `python-dotenv`
  - [ ] `requests`
- [ ] Restart cluster after installation

**Option B: Notebook Installation**

```python
%pip install pyarrow geopandas pyproj shapely scipy python-dotenv requests
dbutils.library.restartPython()
```

### Step 6: Update Notebook Paths

In all notebooks under `databricks_notebooks/`, replace:

```python
# OLD
sys.path.append('/Workspace/Repos/YOUR_EMAIL/nuar_mini_project')

# NEW (example)
sys.path.append('/Workspace/Repos/john.doe@company.com/nuar_mini_project')
```

- [ ] Updated `databricks_notebooks/bronze/01_infrastructure.py`
- [ ] Updated `databricks_notebooks/tests/smoke_test.py`
- [ ] Updated all other notebooks as created

---

## Delta Lake Setup

### Step 7: Create Catalog and Schemas

- [ ] Open `databricks_notebooks/setup_delta_tables.sql`
- [ ] Attach to your cluster
- [ ] Run all cells sequentially
- [ ] Verify output:
  - [ ] Catalog `nuar_catalog` created
  - [ ] Schema `bronze` created
  - [ ] Schema `silver` created
  - [ ] Schema `gold` created
  - [ ] Table `bronze.infrastructure` created
  - [ ] Table `silver.infrastructure` created

**Verification:**

```sql
SHOW CATALOGS;
SHOW SCHEMAS IN nuar_catalog;
SHOW TABLES IN nuar_catalog.bronze;
SHOW TABLES IN nuar_catalog.silver;
```

---

## Testing & Validation

### Step 8: Run Smoke Test

- [ ] Open `databricks_notebooks/tests/smoke_test.py`
- [ ] Attach to cluster
- [ ] Run all cells
- [ ] Verify all tests pass:
  - [ ] âœ… Catalog and schemas exist
  - [ ] âœ… All APIs accessible
  - [ ] âœ… Secrets configured
  - [ ] âœ… Libraries installed
  - [ ] âœ… Configuration loads correctly

### Step 9: Test Bronze Layer

- [ ] Open `databricks_notebooks/bronze/01_infrastructure.py`
- [ ] Run all cells
- [ ] Verify:
  - [ ] API query successful
  - [ ] Data saved to `bronze.infrastructure` table
  - [ ] Record count > 0
  - [ ] Sample data displays correctly
  - [ ] Partitions created correctly

**Quick Check:**

```sql
SELECT
  infrastructure_type,
  COUNT(*) as count
FROM nuar_catalog.bronze.infrastructure
GROUP BY infrastructure_type;
```

---

## Data Pipeline Deployment

### Step 10: Deploy Bronze Layer Notebooks

Create and test each Bronze layer notebook:

- [ ] `databricks_notebooks/bronze/01_infrastructure.py` - Working
- [ ] `databricks_notebooks/bronze/02_crime.py` - Create from template
- [ ] `databricks_notebooks/bronze/03_weather.py` - Create from template
- [ ] `databricks_notebooks/bronze/04_postcodes.py` - Create from template

Run in sequence and verify:

- [ ] All APIs accessible
- [ ] Data ingested successfully
- [ ] No errors or timeouts
- [ ] Data visible in Delta tables

### Step 11: Deploy Silver Layer Notebooks

Create and test each Silver layer notebook:

- [ ] `databricks_notebooks/silver/05_infrastructure.py` - Create from template
- [ ] `databricks_notebooks/silver/06_crime.py` - Create from template
- [ ] `databricks_notebooks/silver/07_weather.py` - Create from template

Verify transformations:

- [ ] Coordinate transformations successful
- [ ] Data quality flags added
- [ ] No data loss
- [ ] BNG coordinates calculated

---

## Workflow Orchestration

### Step 12: Create Databricks Workflow

- [ ] Navigate to "Workflows" in sidebar
- [ ] Click "Create Job"
- [ ] Name: `NUAR Daily Data Pipeline`
- [ ] Add tasks in sequence:
  1. [ ] Bronze Infrastructure
  2. [ ] Bronze Crime
  3. [ ] Bronze Weather
  4. [ ] Bronze Postcodes
  5. [ ] Silver Infrastructure
  6. [ ] Silver Crime
  7. [ ] Silver Weather
- [ ] Set dependencies between tasks
- [ ] Configure schedule: Daily at 2 AM (or as needed)
- [ ] Set notification email for failures
- [ ] Test run workflow manually
- [ ] Verify all tasks complete successfully

---

## Monitoring & Optimization

### Step 13: Performance Optimization

- [ ] Run OPTIMIZE on all tables:

```sql
OPTIMIZE nuar_catalog.bronze.infrastructure;
OPTIMIZE nuar_catalog.silver.infrastructure;
```

- [ ] Add Z-ordering for common query patterns:

```sql
OPTIMIZE nuar_catalog.silver.infrastructure
ZORDER BY (infrastructure_type, easting_bng);
```

- [ ] Set up table maintenance:

```sql
-- Vacuum old files (7 days retention)
VACUUM nuar_catalog.bronze.infrastructure RETAIN 168 HOURS;
```

### Step 14: Monitoring Setup

- [ ] Review cluster metrics
- [ ] Check job run history
- [ ] Monitor table sizes:

```sql
SELECT
  table_schema,
  table_name,
  size_bytes / 1024 / 1024 AS size_mb
FROM system.information_schema.tables
WHERE table_catalog = 'nuar_catalog'
ORDER BY size_bytes DESC;
```

- [ ] Set up alerts for:
  - [ ] Job failures
  - [ ] Long-running queries
  - [ ] Storage size thresholds

### Step 15: Documentation

- [ ] Document cluster configuration
- [ ] Document workflow schedule
- [ ] Create runbook for common issues
- [ ] Document table schemas
- [ ] Share access with team members

---

## Post-Deployment

### âœ… Validation Checklist

- [ ] All Bronze layer tables populated
- [ ] All Silver layer tables populated
- [ ] Data quality metrics within acceptable ranges
- [ ] Workflow runs successfully on schedule
- [ ] Team members can access workspace
- [ ] Dashboards created (if applicable)
- [ ] Documentation complete

### ðŸŽ¯ Success Criteria

- âœ… All 4 API data sources ingesting correctly
- âœ… Bronze â†’ Silver transformations working
- âœ… Coordinate transformations accurate
- âœ… No data loss during transformations
- âœ… Automated workflow running daily
- âœ… Costs within expected range
- âœ… Response time acceptable for queries

---

## Rollback Plan

If issues arise:

1. **Stop Workflow**
   - Pause scheduled runs
   - Prevent data corruption

2. **Restore from Time Travel**
   ```sql
   -- Restore table to previous version
   RESTORE TABLE nuar_catalog.bronze.infrastructure
   TO VERSION AS OF 123;
   ```

3. **Revert Code Changes**
   - Use Git to revert commits
   - Re-sync Databricks Repos

4. **Document Issues**
   - Log errors and symptoms
   - Note what was attempted
   - Share with team

---

## Support Resources

- **Databricks Documentation**: [https://docs.databricks.com](https://docs.databricks.com)
- **Delta Lake Guide**: [https://docs.delta.io](https://docs.delta.io)
- **Project Documentation**:
  - `README.md` - Project overview
  - `docs/DATABRICKS_MIGRATION.md` - Detailed migration guide
  - `databricks_notebooks/DATABRICKS_QUICKSTART.md` - Quick start

- **Community Support**:
  - Databricks Community: [https://community.databricks.com](https://community.databricks.com)
  - Stack Overflow: Tag `databricks`

---

## ðŸŽ‰ Deployment Complete!

Once all items are checked:

1. Celebrate! ðŸŽŠ
2. Monitor first few automated runs
3. Share success with team
4. Plan Gold layer development
5. Consider dashboard creation

---

**Last Updated**: 2025-10-27
**Version**: 1.0.0

**Deployed By**: ___________________
**Deployment Date**: ___________________
**Workspace URL**: ___________________
