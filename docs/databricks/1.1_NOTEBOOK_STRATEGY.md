# 📓 Notebook Strategy: Local vs Databricks

## Overview

This project now has **TWO sets of notebooks** for different environments:

1. **`notebooks/`** - Local development notebooks (keep these!)
2. **`databricks_notebooks/`** - Databricks-compatible versions (auto-converted)

---

## 📁 Directory Structure

```text
nuar_mini_project/
├── notebooks/                          # LOCAL DEVELOPMENT
│   ├── 01_bronze_infrastructure_full.py    ✅ Working locally
│   ├── 02_bronze_crime_full.py             ✅ Working locally
│   ├── 03_bronze_weather.py                ✅ Working locally
│   ├── 04_bronze_postcodes_full.py         ✅ Working locally
│   ├── 05_silver_infrastructure.py         ✅ Working locally
│   ├── 06_silver_crime.py                  ✅ Working locally
│   └── 07_silver_weather.py                ✅ Working locally
│
└── databricks_notebooks/               # DATABRICKS DEPLOYMENT
    ├── bronze/
    │   ├── 01_bronze_infrastructure_full.py   🔄 Databricks version
    │   ├── 02_bronze_crime_full.py            🔄 Databricks version
    │   ├── 03_bronze_weather.py               🔄 Databricks version
    │   └── 04_bronze_postcodes_full.py        🔄 Databricks version
    ├── silver/
    │   ├── 05_silver_infrastructure.py        🔄 Databricks version
    │   ├── 06_silver_crime.py                 🔄 Databricks version
    │   └── 07_silver_weather.py               🔄 Databricks version
    ├── tests/
    │   └── smoke_test.py                      🆕 Databricks validation
    ├── setup_delta_tables.sql                 🆕 Delta Lake setup
    └── DATABRICKS_QUICKSTART.md               🆕 Quick start guide
```

---

## 🎯 Strategy: Keep Both!

### Use `notebooks/` for:

- ✅ **Local development and testing**
- ✅ **Rapid prototyping**
- ✅ **Data exploration**
- ✅ **Running without Databricks costs**
- ✅ **Version control and code review**
- ✅ **Initial data collection**

**File Paths:**
- Reads/writes to `data/bronze/`, `data/silver/`, `data/gold/`
- Uses local Parquet files
- Imports from `src/` using relative paths

### Use `databricks_notebooks/` for:

- ✅ **Production data pipeline**
- ✅ **Scheduled workflows**
- ✅ **Large-scale processing**
- ✅ **Team collaboration**
- ✅ **Delta Lake tables**
- ✅ **Databricks-specific features**

**File Paths:**
- Reads/writes to Delta tables: `nuar_catalog.bronze.*`, `nuar_catalog.silver.*`
- Uses DBFS: `dbfs:/FileStore/nuar/`
- Imports from Databricks Repos

---

## 🔄 Key Differences

### Local Notebooks (`notebooks/`)

```python
# Import from local src/
import sys
from pathlib import Path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.utils.api_client import OverpassClient

# Load config from file
with open(project_root / 'config' / 'stockport.json') as f:
    config = json.load(f)

# Save to local Parquet
output_dir = project_root / 'data' / 'bronze' / 'stockport' / 'infrastructure'
df.to_parquet(output_dir / 'data.parquet')
```

### Databricks Notebooks (`databricks_notebooks/`)

```python
# Databricks notebook source
# MAGIC %md
# MAGIC # Bronze Layer - Infrastructure Data

# COMMAND ----------
# Install libraries
%pip install pyarrow geopandas pyproj shapely scipy

# COMMAND ----------
dbutils.library.restartPython()

# COMMAND ----------
# Import from Databricks Repos
import sys
sys.path.append('/Workspace/Repos/YOUR_EMAIL/nuar_mini_project')

from config.databricks_settings import *
from src.utils.api_client import OverpassClient

# COMMAND ----------
# Configuration from databricks_settings
config = STOCKPORT_CONFIG

# COMMAND ----------
# Save to Delta Lake
spark_df = spark.createDataFrame(df)
spark_df.write \
    .format("delta") \
    .mode("append") \
    .saveAsTable("nuar_catalog.bronze.infrastructure")

# COMMAND ----------
# Display results
display(spark.table("nuar_catalog.bronze.infrastructure").limit(10))
```

---

## 🚀 Workflow Recommendation

### Development Workflow

1. **Develop locally** using `notebooks/`:
   ```bash
   python notebooks/01_bronze_infrastructure_full.py
   ```

2. **Test and validate** with local data

3. **Convert to Databricks** when ready:
   ```bash
   python convert_notebooks_to_databricks.py
   ```

4. **Review converted notebooks** in `databricks_notebooks/`

5. **Update paths** (replace `YOUR_EMAIL`)

6. **Push to GitHub**:
   ```bash
   git add databricks_notebooks/
   git commit -m "feat: Add Databricks-compatible notebooks"
   git push
   ```

7. **Deploy to Databricks** via Repos

8. **Run in Databricks** with Delta Lake

### Production Workflow

1. **Schedule workflows** in Databricks using `databricks_notebooks/`
2. **Monitor execution** via Databricks UI
3. **Query results** from Delta tables
4. **Make updates** to local `notebooks/` first
5. **Re-convert** when changes are ready
6. **Deploy** via Git push → Databricks Repos sync

---

## 📊 Conversion Details

The `convert_notebooks_to_databricks.py` script automatically:

### ✅ Adds Databricks Headers
- Cell separators (`# COMMAND ----------`)
- Markdown cells (`# MAGIC %md`)
- Installation cells

### ✅ Converts Imports
```python
# FROM (local)
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# TO (Databricks)
sys.path.append('/Workspace/Repos/YOUR_EMAIL/nuar_mini_project')
from config.databricks_settings import *
```

### ✅ Updates File Paths
```python
# FROM (local)
df = pd.read_parquet('data/bronze/stockport/infrastructure/')

# TO (Databricks)
df = spark.table(BRONZE_TABLES['infrastructure']).toPandas()
```

### ✅ Adds Delta Lake Operations
```python
# Saves to both Parquet AND Delta
spark_df = spark.createDataFrame(df)
spark_df.write.format("delta").saveAsTable(table_name)
```

### ✅ Adds Display Commands
```python
# Better visualization in Databricks
display(spark.table("nuar_catalog.bronze.infrastructure"))
```

---

## 🔧 Manual Adjustments Needed

After conversion, you must update:

### 1. Replace Email in All Databricks Notebooks

Find and replace in `databricks_notebooks/**/*.py`:

```python
# FIND:
sys.path.append('/Workspace/Repos/YOUR_EMAIL/nuar_mini_project')

# REPLACE WITH (example):
sys.path.append('/Workspace/Repos/john.doe@company.com/nuar_mini_project')
```

### 2. Test Each Notebook in Databricks

- Attach to cluster
- Run all cells
- Verify data in Delta tables
- Check for any import errors

### 3. Adjust Table Names if Needed

If you use different catalog/schema names:

```python
# In databricks_settings.py, update:
CATALOG_NAME = "your_catalog"
BRONZE_SCHEMA = "your_bronze_schema"
```

---

## 📝 Best Practices

### ✅ DO:

- Keep both notebook sets in sync
- Develop locally first
- Test locally before deploying
- Use version control for both
- Document any manual changes to Databricks versions
- Re-run conversion script after major local changes

### ❌ DON'T:

- Edit Databricks notebooks directly (they get overwritten)
- Delete local notebooks (they're your source of truth)
- Commit secrets or API keys
- Mix local and Databricks paths in same notebook

---

## 🔄 Re-Converting After Changes

When you update local notebooks:

```bash
# Make changes to notebooks/*.py
vim notebooks/01_bronze_infrastructure_full.py

# Test locally
python notebooks/01_bronze_infrastructure_full.py

# Re-convert for Databricks
python convert_notebooks_to_databricks.py

# Review changes
git diff databricks_notebooks/

# Commit and push
git add notebooks/ databricks_notebooks/
git commit -m "feat: Update infrastructure collection logic"
git push

# In Databricks: Pull latest from Repos
```

---

## 📈 Migration Path

### Phase 1: Local Development (Current)
- ✅ All notebooks working locally
- ✅ Data in `data/bronze/` and `data/silver/`
- ✅ Rapid iteration

### Phase 2: Databricks Setup (Next)
- 🔄 Convert notebooks
- 🔄 Deploy to Databricks
- 🔄 Create Delta tables
- 🔄 Run smoke tests

### Phase 3: Hybrid Operation
- 🔄 Develop locally in `notebooks/`
- 🔄 Deploy to Databricks via `databricks_notebooks/`
- 🔄 Production data in Delta Lake
- 🔄 Local testing with samples

### Phase 4: Full Production
- 🔄 Scheduled Databricks workflows
- 🔄 Delta tables as source of truth
- 🔄 Local notebooks for prototyping only
- 🔄 Dashboards and analytics

---

## 🆘 Troubleshooting

### "Module not found" in Databricks

```python
# Check sys.path
import sys
print(sys.path)

# Verify Repos path
%sh ls -la /Workspace/Repos/YOUR_EMAIL/nuar_mini_project
```

### "Table not found" errors

```sql
-- Verify catalog/schema
SHOW CATALOGS;
USE CATALOG nuar_catalog;
SHOW SCHEMAS;
SHOW TABLES IN bronze;
```

### Want to re-convert single notebook

```python
# Edit convert_notebooks_to_databricks.py
# Update notebook_files filter:
notebook_files = [notebooks_dir / '01_bronze_infrastructure_full.py']
```

---

## 📚 Summary

**Keep BOTH sets of notebooks:**

- 📁 `notebooks/` = Your working source of truth
- 📁 `databricks_notebooks/` = Auto-generated Databricks versions

**Workflow:**
1. Develop → Test → Convert → Deploy → Monitor
2. Make changes locally first
3. Re-convert when ready
4. Deploy via Git

**This gives you:**
- ✅ Fast local development
- ✅ Easy Databricks deployment
- ✅ Version control for both
- ✅ Best of both worlds!

---

**Last Updated**: 2025-10-27
**Conversion Script**: `convert_notebooks_to_databricks.py`
