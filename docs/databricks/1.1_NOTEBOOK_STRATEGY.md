# ğŸ““ Notebook Strategy: Local vs Databricks

## Overview

This project now has **TWO sets of notebooks** for different environments:

1. **`notebooks/`** - Local development notebooks (keep these!)
2. **`databricks_notebooks/`** - Databricks-compatible versions (auto-converted)

---

## ğŸ“ Directory Structure

```text
nuar_mini_project/
â”œâ”€â”€ notebooks/                          # LOCAL DEVELOPMENT
â”‚   â”œâ”€â”€ 01_bronze_infrastructure_full.py    âœ… Working locally
â”‚   â”œâ”€â”€ 02_bronze_crime_full.py             âœ… Working locally
â”‚   â”œâ”€â”€ 03_bronze_weather.py                âœ… Working locally
â”‚   â”œâ”€â”€ 04_bronze_postcodes_full.py         âœ… Working locally
â”‚   â”œâ”€â”€ 05_silver_infrastructure.py         âœ… Working locally
â”‚   â”œâ”€â”€ 06_silver_crime.py                  âœ… Working locally
â”‚   â””â”€â”€ 07_silver_weather.py                âœ… Working locally
â”‚
â””â”€â”€ databricks_notebooks/               # DATABRICKS DEPLOYMENT
    â”œâ”€â”€ bronze/
    â”‚   â”œâ”€â”€ 01_bronze_infrastructure_full.py   ğŸ”„ Databricks version
    â”‚   â”œâ”€â”€ 02_bronze_crime_full.py            ğŸ”„ Databricks version
    â”‚   â”œâ”€â”€ 03_bronze_weather.py               ğŸ”„ Databricks version
    â”‚   â””â”€â”€ 04_bronze_postcodes_full.py        ğŸ”„ Databricks version
    â”œâ”€â”€ silver/
    â”‚   â”œâ”€â”€ 05_silver_infrastructure.py        ğŸ”„ Databricks version
    â”‚   â”œâ”€â”€ 06_silver_crime.py                 ğŸ”„ Databricks version
    â”‚   â””â”€â”€ 07_silver_weather.py               ğŸ”„ Databricks version
    â”œâ”€â”€ tests/
    â”‚   â””â”€â”€ smoke_test.py                      ğŸ†• Databricks validation
    â”œâ”€â”€ setup_delta_tables.sql                 ğŸ†• Delta Lake setup
    â””â”€â”€ DATABRICKS_QUICKSTART.md               ğŸ†• Quick start guide
```

---

## ğŸ¯ Strategy: Keep Both!

### Use `notebooks/` for:

- âœ… **Local development and testing**
- âœ… **Rapid prototyping**
- âœ… **Data exploration**
- âœ… **Running without Databricks costs**
- âœ… **Version control and code review**
- âœ… **Initial data collection**

**File Paths:**
- Reads/writes to `data/bronze/`, `data/silver/`, `data/gold/`
- Uses local Parquet files
- Imports from `src/` using relative paths

### Use `databricks_notebooks/` for:

- âœ… **Production data pipeline**
- âœ… **Scheduled workflows**
- âœ… **Large-scale processing**
- âœ… **Team collaboration**
- âœ… **Delta Lake tables**
- âœ… **Databricks-specific features**

**File Paths:**
- Reads/writes to Delta tables: `nuar_catalog.bronze.*`, `nuar_catalog.silver.*`
- Uses DBFS: `dbfs:/FileStore/nuar/`
- Imports from Databricks Repos

---

## ğŸ”„ Key Differences

### Local Notebooks (`notebooks/`)

```python
# Import from local src/
import sys
from pathlib import Path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.utils.api_client import OverpassClient

# Load config from file
with open(project_root / 'config' / 'stockport.json') as f:
    config = json.load(f)

# Save to local Parquet
output_dir = project_root / 'data' / 'bronze' / 'stockport' / 'infrastructure'
df.to_parquet(output_dir / 'data.parquet')
```

### Databricks Notebooks (`databricks_notebooks/`)

```python
# Databricks notebook source
# MAGIC %md
# MAGIC # Bronze Layer - Infrastructure Data

# COMMAND ----------
# Install libraries
%pip install pyarrow geopandas pyproj shapely scipy

# COMMAND ----------
dbutils.library.restartPython()

# COMMAND ----------
# Import from Databricks Repos
import sys
sys.path.append('/Workspace/Repos/YOUR_EMAIL/nuar_mini_project')

from config.databricks_settings import *
from src.utils.api_client import OverpassClient

# COMMAND ----------
# Configuration from databricks_settings
config = STOCKPORT_CONFIG

# COMMAND ----------
# Save to Delta Lake
spark_df = spark.createDataFrame(df)
spark_df.write \
    .format("delta") \
    .mode("append") \
    .saveAsTable("nuar_catalog.bronze.infrastructure")

# COMMAND ----------
# Display results
display(spark.table("nuar_catalog.bronze.infrastructure").limit(10))
```

---

## ğŸš€ Workflow Recommendation

### Development Workflow

1. **Develop locally** using `notebooks/`:
   ```bash
   python notebooks/01_bronze_infrastructure_full.py
   ```

2. **Test and validate** with local data

3. **Convert to Databricks** when ready:
   ```bash
   python convert_notebooks_to_databricks.py
   ```

4. **Review converted notebooks** in `databricks_notebooks/`

5. **Update paths** (replace `YOUR_EMAIL`)

6. **Push to GitHub**:
   ```bash
   git add databricks_notebooks/
   git commit -m "feat: Add Databricks-compatible notebooks"
   git push
   ```

7. **Deploy to Databricks** via Repos

8. **Run in Databricks** with Delta Lake

### Production Workflow

1. **Schedule workflows** in Databricks using `databricks_notebooks/`
2. **Monitor execution** via Databricks UI
3. **Query results** from Delta tables
4. **Make updates** to local `notebooks/` first
5. **Re-convert** when changes are ready
6. **Deploy** via Git push â†’ Databricks Repos sync

---

## ğŸ“Š Conversion Details

The `convert_notebooks_to_databricks.py` script automatically:

### âœ… Adds Databricks Headers
- Cell separators (`# COMMAND ----------`)
- Markdown cells (`# MAGIC %md`)
- Installation cells

### âœ… Converts Imports
```python
# FROM (local)
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# TO (Databricks)
sys.path.append('/Workspace/Repos/YOUR_EMAIL/nuar_mini_project')
from config.databricks_settings import *
```

### âœ… Updates File Paths
```python
# FROM (local)
df = pd.read_parquet('data/bronze/stockport/infrastructure/')

# TO (Databricks)
df = spark.table(BRONZE_TABLES['infrastructure']).toPandas()
```

### âœ… Adds Delta Lake Operations
```python
# Saves to both Parquet AND Delta
spark_df = spark.createDataFrame(df)
spark_df.write.format("delta").saveAsTable(table_name)
```

### âœ… Adds Display Commands
```python
# Better visualization in Databricks
display(spark.table("nuar_catalog.bronze.infrastructure"))
```

---

## ğŸ”§ Manual Adjustments Needed

After conversion, you must update:

### 1. Replace Email in All Databricks Notebooks

Find and replace in `databricks_notebooks/**/*.py`:

```python
# FIND:
sys.path.append('/Workspace/Repos/YOUR_EMAIL/nuar_mini_project')

# REPLACE WITH (example):
sys.path.append('/Workspace/Repos/john.doe@company.com/nuar_mini_project')
```

### 2. Test Each Notebook in Databricks

- Attach to cluster
- Run all cells
- Verify data in Delta tables
- Check for any import errors

### 3. Adjust Table Names if Needed

If you use different catalog/schema names:

```python
# In databricks_settings.py, update:
CATALOG_NAME = "your_catalog"
BRONZE_SCHEMA = "your_bronze_schema"
```

---

## ğŸ“ Best Practices

### âœ… DO:

- Keep both notebook sets in sync
- Develop locally first
- Test locally before deploying
- Use version control for both
- Document any manual changes to Databricks versions
- Re-run conversion script after major local changes

### âŒ DON'T:

- Edit Databricks notebooks directly (they get overwritten)
- Delete local notebooks (they're your source of truth)
- Commit secrets or API keys
- Mix local and Databricks paths in same notebook

---

## ğŸ”„ Re-Converting After Changes

When you update local notebooks:

```bash
# Make changes to notebooks/*.py
vim notebooks/01_bronze_infrastructure_full.py

# Test locally
python notebooks/01_bronze_infrastructure_full.py

# Re-convert for Databricks
python convert_notebooks_to_databricks.py

# Review changes
git diff databricks_notebooks/

# Commit and push
git add notebooks/ databricks_notebooks/
git commit -m "feat: Update infrastructure collection logic"
git push

# In Databricks: Pull latest from Repos
```

---

## ğŸ“ˆ Migration Path

### Phase 1: Local Development (Current)
- âœ… All notebooks working locally
- âœ… Data in `data/bronze/` and `data/silver/`
- âœ… Rapid iteration

### Phase 2: Databricks Setup (Next)
- ğŸ”„ Convert notebooks
- ğŸ”„ Deploy to Databricks
- ğŸ”„ Create Delta tables
- ğŸ”„ Run smoke tests

### Phase 3: Hybrid Operation
- ğŸ”„ Develop locally in `notebooks/`
- ğŸ”„ Deploy to Databricks via `databricks_notebooks/`
- ğŸ”„ Production data in Delta Lake
- ğŸ”„ Local testing with samples

### Phase 4: Full Production
- ğŸ”„ Scheduled Databricks workflows
- ğŸ”„ Delta tables as source of truth
- ğŸ”„ Local notebooks for prototyping only
- ğŸ”„ Dashboards and analytics

---

## ğŸ†˜ Troubleshooting

### "Module not found" in Databricks

```python
# Check sys.path
import sys
print(sys.path)

# Verify Repos path
%sh ls -la /Workspace/Repos/YOUR_EMAIL/nuar_mini_project
```

### "Table not found" errors

```sql
-- Verify catalog/schema
SHOW CATALOGS;
USE CATALOG nuar_catalog;
SHOW SCHEMAS;
SHOW TABLES IN bronze;
```

### Want to re-convert single notebook

```python
# Edit convert_notebooks_to_databricks.py
# Update notebook_files filter:
notebook_files = [notebooks_dir / '01_bronze_infrastructure_full.py']
```

---

## ğŸ“š Summary

**Keep BOTH sets of notebooks:**

- ğŸ“ `notebooks/` = Your working source of truth
- ğŸ“ `databricks_notebooks/` = Auto-generated Databricks versions

**Workflow:**
1. Develop â†’ Test â†’ Convert â†’ Deploy â†’ Monitor
2. Make changes locally first
3. Re-convert when ready
4. Deploy via Git

**This gives you:**
- âœ… Fast local development
- âœ… Easy Databricks deployment
- âœ… Version control for both
- âœ… Best of both worlds!

---

**Last Updated**: 2025-10-27
**Conversion Script**: `convert_notebooks_to_databricks.py`
